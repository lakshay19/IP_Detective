{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6ef7bdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T18:38:16.930564Z",
     "iopub.status.busy": "2022-03-02T18:38:16.930242Z",
     "iopub.status.idle": "2022-03-02T18:38:22.311161Z",
     "shell.execute_reply": "2022-03-02T18:38:22.310393Z",
     "shell.execute_reply.started": "2022-03-02T18:38:16.930501Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity, paired_distances\n",
    "from scipy import spatial\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d523300d-d37c-49a7-ab33-6fe5c8dd067a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T16:33:27.624013Z",
     "iopub.status.busy": "2022-03-02T16:33:27.623722Z",
     "iopub.status.idle": "2022-03-02T16:34:11.447281Z",
     "shell.execute_reply": "2022-03-02T16:34:11.446512Z",
     "shell.execute_reply.started": "2022-03-02T16:33:27.623988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-1.10.2-cp39-cp39-manylinux1_x86_64.whl (881.9 MB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/saturn/lib/python3.9/site-packages (from torch) (3.10.0.2)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.10.2\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
      "Collecting tqdm>=4.27\n",
      "  Using cached tqdm-4.63.0-py2.py3-none-any.whl (76 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/saturn/lib/python3.9/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Using cached huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from transformers) (1.21.2)\n",
      "Collecting tokenizers!=0.11.3,>=0.10.1\n",
      "  Using cached tokenizers-0.11.6-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2022.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (763 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.6.0-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Requirement already satisfied: six in /opt/conda/envs/saturn/lib/python3.9/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/envs/saturn/lib/python3.9/site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Installing collected packages: tqdm, regex, joblib, filelock, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.6.0 huggingface-hub-0.4.0 joblib-1.1.0 regex-2022.3.2 sacremoses-0.0.47 tokenizers-0.11.6 tqdm-4.63.0 transformers-4.16.2\n",
      "Collecting sentence_transformers\n",
      "  Using cached sentence_transformers-2.2.0-py3-none-any.whl\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/saturn/lib/python3.9/site-packages (from sentence_transformers) (4.63.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from sentence_transformers) (1.10.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/saturn/lib/python3.9/site-packages (from sentence_transformers) (1.21.2)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.4 MB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from sentence_transformers) (4.16.2)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/envs/saturn/lib/python3.9/site-packages (from sentence_transformers) (0.4.0)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.96-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.11.3-cp39-cp39-manylinux1_x86_64.whl (23.2 MB)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/saturn/lib/python3.9/site-packages (from sentence_transformers) (1.7.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/saturn/lib/python3.9/site-packages (from torch>=1.6.0->sentence_transformers) (3.10.0.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/saturn/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.3.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/saturn/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/envs/saturn/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.0.47)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.4)\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/saturn/lib/python3.9/site-packages (from nltk->sentence_transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /opt/conda/envs/saturn/lib/python3.9/site-packages (from nltk->sentence_transformers) (8.0.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.26.7)\n",
      "Requirement already satisfied: six in /opt/conda/envs/saturn/lib/python3.9/site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.16.0)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from torchvision->sentence_transformers) (8.4.0)\n",
      "Installing collected packages: threadpoolctl, torchvision, sentencepiece, scikit-learn, nltk, sentence-transformers\n",
      "Successfully installed nltk-3.7 scikit-learn-1.0.2 sentence-transformers-2.2.0 sentencepiece-0.1.96 threadpoolctl-3.1.0 torchvision-0.11.3\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch\n",
    "# !pip install transformers\n",
    "# !pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7a93ae7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T18:39:11.598479Z",
     "iopub.status.busy": "2022-03-02T18:39:11.597993Z",
     "iopub.status.idle": "2022-03-02T18:39:19.566116Z",
     "shell.execute_reply": "2022-03-02T18:39:19.565448Z",
     "shell.execute_reply.started": "2022-03-02T18:39:11.598449Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at anferico/bert-for-patents were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "tokenizer = AutoTokenizer.from_pretrained('anferico/bert-for-patents')\n",
    "model = AutoModel.from_pretrained('anferico/bert-for-patents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8b19e46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T18:39:21.131433Z",
     "iopub.status.busy": "2022-03-02T18:39:21.131139Z",
     "iopub.status.idle": "2022-03-02T18:39:21.145263Z",
     "shell.execute_reply": "2022-03-02T18:39:21.144755Z",
     "shell.execute_reply.started": "2022-03-02T18:39:21.131408Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this function splits the data\n",
    "def get_split(text,length):\n",
    "    l_total = []\n",
    "    l_parcial = []\n",
    "    n = len(text.split())//length\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        l_parcial = text.split()[i*length:(i+1)*length]\n",
    "        l_total.append(\" \".join(l_parcial))\n",
    "        i+=1\n",
    "        \n",
    "    l_parcial = text.split()[i*length:]\n",
    "    l_total.append(\" \".join(l_parcial))\n",
    "        \n",
    "    return l_total\n",
    "\n",
    "\n",
    "# this function is for pooling\n",
    "def cls_pooling(model_output, attention_mask):\n",
    "    return model_output[0][:,0]\n",
    "\n",
    "# this function generates similarity score from two text\n",
    "def get_sim_score(text1,text2,tokenizer,model):\n",
    "    # prepare the input texts\n",
    "    len_of_split = 300\n",
    "    text1 = get_split(str(text1),len_of_split)\n",
    "    text2 = get_split(str(text2),len_of_split)\n",
    "    sep = len(text1)\n",
    "    combined_text = text1+text2\n",
    "    \n",
    "    # generate the vector sets\n",
    "    encoded_input = tokenizer(combined_text, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    combined_vec = cls_pooling(model_output, encoded_input['attention_mask'])\n",
    "    \n",
    "    # compare each part of two texts and calculate the similarity score\n",
    "    p1 = combined_vec[:sep]\n",
    "    p2 = combined_vec[sep:]\n",
    "    similarity_score = 0\n",
    "    for i in p1:\n",
    "        for j in p2:\n",
    "            cosine_sim = 1 - spatial.distance.cosine(i, j)\n",
    "            if cosine_sim>similarity_score:\n",
    "                similarity_score = cosine_sim\n",
    "\n",
    "\n",
    "    return similarity_score\n",
    "\n",
    "\n",
    "# this function transform the text in the patent dictionary into vec\n",
    "def generate_vec_for_one(patent,tokenizer,model):\n",
    "    # save all the splited texts\n",
    "    content_all = []\n",
    "    # record the key and length of splited texts\n",
    "    ref_list = []\n",
    "    # traverse the plaintiff\n",
    "    patent_length = 0\n",
    "    for key,value in patent.items():\n",
    "        content = get_split(str(value),300)\n",
    "        length = len(content)\n",
    "        content_all+=content\n",
    "        ref_list.append((key,length))\n",
    "        patent_length+=length\n",
    "    \n",
    "    # calculate the vec\n",
    "    encoded_input = tokenizer(content_all, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    content_vec = cls_pooling(model_output, encoded_input['attention_mask'])\n",
    "    \n",
    "    # write the vec back to dict\n",
    "    patent_vec_dict = {}\n",
    "\n",
    "    idx = 0\n",
    "    i = 0\n",
    "    while idx < patent_length: \n",
    "        key = ref_list[i][0]\n",
    "        length = ref_list[i][1]\n",
    "        content = content_vec[idx:idx+length]\n",
    "        patent_vec_dict[key] = content\n",
    "        idx +=length\n",
    "        i+=1\n",
    "\n",
    "    \n",
    "    return patent_vec_dict\n",
    "\n",
    "\n",
    "# this function generates a dataframe of similarity scores of different combination of each parts of plantiff and defendant patent \n",
    "def compare_two_patents(plaintiff_no,defendant_no,data_dict,tokenizer,model):\n",
    "    score_dict = {'Plaintiff_Patent':[],'Defendant_Patent':[],'Plaintiff_Category':[],'Defendant_Category':[],'Similarity_Score':[]}\n",
    "    plaintiff = data_dict[plaintiff_no]\n",
    "    defendant = data_dict[defendant_no]\n",
    "    p_vec, d_vec = generate_vec(plaintiff,defendant,tokenizer,model)\n",
    "    \n",
    "    # generate the df\n",
    "    for keyp,valuep in p_vec.items():\n",
    "        for keyd,valued in d_vec.items():\n",
    "\n",
    "            # calculate the max for the combination\n",
    "            sim_score = 0\n",
    "            for vec1 in valuep:\n",
    "                for vec2 in valued:\n",
    "                    cosine_sim = 1 - spatial.distance.cosine(vec1, vec2)\n",
    "                    if cosine_sim > sim_score:\n",
    "                        sim_score = cosine_sim\n",
    "\n",
    "            score_dict['Plaintiff_Patent'].append(plaintiff_no)\n",
    "            score_dict['Defendant_Patent'].append(defendant_no)\n",
    "            score_dict['Plaintiff_Category'].append(keyp)\n",
    "            score_dict['Defendant_Category'].append(keyd)\n",
    "            score_dict['Similarity_Score'].append(sim_score)\n",
    "            \n",
    "    score_df = pd.DataFrame(score_dict)\n",
    "    return score_df\n",
    "\n",
    "# this function use less memory but much longer time \n",
    "def compare_two_patents_less_memo(plaintiff_no,defendant_no,data_dict):\n",
    "    score_dict = {'Plaintiff_Patent':[],'Defendant_Patent':[],'Plaintiff_Category':[],'Defendant_Category':[],'Similarity_Score':[]}\n",
    "    plaintiff = data_dict[plaintiff_no]\n",
    "    defendant = data_dict[defendant_no]\n",
    "    for keyp,valuep in plaintiff.items():\n",
    "        for keyd,valued in defendant.items():\n",
    "            score_dict['Plaintiff_Patent'].append(plaintiff_no)\n",
    "            score_dict['Defendant_Patent'].append(defendant_no)\n",
    "            score_dict['Plaintiff_Category'].append(keyp)\n",
    "            \n",
    "            score_dict['Defendant_Category'].append(keyd)\n",
    "            score = get_sim_score(valuep,valued,tokenizer,model)\n",
    "            score_dict['Similarity_Score'].append(score)\n",
    "            \n",
    "    score_df = pd.DataFrame(score_dict)\n",
    "    return score_df\n",
    "\n",
    "# this function is the advanced version of the previous one with dict\n",
    "def compare_two_patents_with_dict(plaintiff_no,defendant_no,data_dict,tokenizer,model):\n",
    "    score_dict = {'Plaintiff_Patent':[],'Defendant_Patent':[],'Plaintiff_Category':[],'Defendant_Category':[],'Similarity_Score':[]}\n",
    "    plaintiff = data_dict[plaintiff_no]\n",
    "    defendant = data_dict[defendant_no]\n",
    "    \n",
    "    # generate the vec for patents\n",
    "    if plaintiff_no in patent_vec_dict:\n",
    "        p_vec = patent_vec_dict[plaintiff_no]\n",
    "    else:\n",
    "        p_vec = generate_vec_for_one(plaintiff,tokenizer,model)\n",
    "        patent_vec_dict[plaintiff_no] = p_vec\n",
    "        \n",
    "    if defendant_no in patent_vec_dict:\n",
    "        d_vec = patent_vec_dict[defendant_no]\n",
    "    else:\n",
    "        d_vec = generate_vec_for_one(defendant,tokenizer,model)\n",
    "        patent_vec_dict[defendant_no] = d_vec\n",
    "    \n",
    "    \n",
    "    # generate the df\n",
    "    for keyp,valuep in p_vec.items():\n",
    "        for keyd,valued in d_vec.items():\n",
    "\n",
    "            # calculate the max for the combination\n",
    "            sim_score = 0\n",
    "            for vec1 in valuep:\n",
    "                for vec2 in valued:\n",
    "                    cosine_sim = 1 - spatial.distance.cosine(vec1, vec2)\n",
    "                    if cosine_sim > sim_score:\n",
    "                        sim_score = cosine_sim\n",
    "\n",
    "            score_dict['Plaintiff_Patent'].append(plaintiff_no)\n",
    "            score_dict['Defendant_Patent'].append(defendant_no)\n",
    "            score_dict['Plaintiff_Category'].append(keyp)\n",
    "            score_dict['Defendant_Category'].append(keyd)\n",
    "            score_dict['Similarity_Score'].append(sim_score)\n",
    "            \n",
    "    score_df = pd.DataFrame(score_dict)\n",
    "    return score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d33f23ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T18:39:21.895568Z",
     "iopub.status.busy": "2022-03-02T18:39:21.895302Z",
     "iopub.status.idle": "2022-03-02T18:39:28.914204Z",
     "shell.execute_reply": "2022-03-02T18:39:28.913354Z",
     "shell.execute_reply.started": "2022-03-02T18:39:21.895546Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data dictionary\n",
    "with open(\"patent_data.json\",'r', encoding='UTF-8') as f:\n",
    "     patent_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5aea10e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T18:39:28.922637Z",
     "iopub.status.busy": "2022-03-02T18:39:28.922401Z",
     "iopub.status.idle": "2022-03-02T18:39:28.932220Z",
     "shell.execute_reply": "2022-03-02T18:39:28.931758Z",
     "shell.execute_reply.started": "2022-03-02T18:39:28.922607Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the plantiff and defendant patent no.\n",
    "df_train_no = pd.read_csv('Simplified_training_data_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7d6448b-620e-4e41-8fdd-332de804bfb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T18:39:38.459218Z",
     "iopub.status.busy": "2022-03-02T18:39:38.458924Z",
     "iopub.status.idle": "2022-03-02T18:39:38.468511Z",
     "shell.execute_reply": "2022-03-02T18:39:38.467900Z",
     "shell.execute_reply.started": "2022-03-02T18:39:38.459195Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>plaintiff_patent</th>\n",
       "      <th>defendant_patent</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>US6027520A</td>\n",
       "      <td>US7976648B1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>US6027520A</td>\n",
       "      <td>US6582448B1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>US6027520A</td>\n",
       "      <td>US6855161B2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>US6027520A</td>\n",
       "      <td>US6569184B2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>US6027520A</td>\n",
       "      <td>US6746469B2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>US6027520A</td>\n",
       "      <td>US6551341B2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>US6027520A</td>\n",
       "      <td>US6575996B1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>US6027520A</td>\n",
       "      <td>US7344549B2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>US6027520A</td>\n",
       "      <td>US7942892B2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>US6027520A</td>\n",
       "      <td>US20090105644A1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>US6027520A</td>\n",
       "      <td>US20100168786A1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>US6027520A</td>\n",
       "      <td>EP1292356A2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>US6027520A</td>\n",
       "      <td>WO2003034942A1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>US6027520A</td>\n",
       "      <td>US6740040B1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>US6027520A</td>\n",
       "      <td>US6837898B2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>US6027520A</td>\n",
       "      <td>US20060129183A1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>US6027520A</td>\n",
       "      <td>US7241304B2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>US6027520A</td>\n",
       "      <td>US20080215084A1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>US6027520A</td>\n",
       "      <td>US20100004674A1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>US6027520A</td>\n",
       "      <td>US20100185179A1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    plaintiff_patent defendant_patent  Label\n",
       "350       US6027520A      US7976648B1   True\n",
       "351       US6027520A      US6582448B1   True\n",
       "352       US6027520A      US6855161B2   True\n",
       "353       US6027520A      US6569184B2   True\n",
       "354       US6027520A      US6746469B2   True\n",
       "355       US6027520A      US6551341B2   True\n",
       "356       US6027520A      US6575996B1   True\n",
       "357       US6027520A      US7344549B2   True\n",
       "358       US6027520A      US7942892B2   True\n",
       "359       US6027520A  US20090105644A1   True\n",
       "360       US6027520A  US20100168786A1   True\n",
       "361       US6027520A      EP1292356A2   True\n",
       "362       US6027520A   WO2003034942A1   True\n",
       "363       US6027520A      US6740040B1   True\n",
       "364       US6027520A      US6837898B2   True\n",
       "365       US6027520A  US20060129183A1   True\n",
       "366       US6027520A      US7241304B2   True\n",
       "367       US6027520A  US20080215084A1   True\n",
       "368       US6027520A  US20100004674A1   True\n",
       "369       US6027520A  US20100185179A1   True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate the df for the first 50 pairs\n",
    "df_train_no = df_train_no[350:370]\n",
    "df_train_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2453cec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the dict to score vec\n",
    "patent_vec_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c4110ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T18:39:46.670379Z",
     "iopub.status.busy": "2022-03-02T18:39:46.670067Z",
     "iopub.status.idle": "2022-03-02T19:45:40.780906Z",
     "shell.execute_reply": "2022-03-02T19:45:40.780266Z",
     "shell.execute_reply.started": "2022-03-02T18:39:46.670355Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now start the  1  th pair calculation.\n",
      "1 th has finished.\n",
      "Now start the  2  th pair calculation.\n",
      "2 th has finished.\n",
      "Now start the  3  th pair calculation.\n",
      "3 th has finished.\n",
      "Now start the  4  th pair calculation.\n",
      "4 th has finished.\n",
      "Now start the  5  th pair calculation.\n",
      "5 th has finished.\n",
      "Now start the  6  th pair calculation.\n",
      "6 th has finished.\n",
      "Now start the  7  th pair calculation.\n",
      "7 th has finished.\n",
      "Now start the  8  th pair calculation.\n",
      "8 th has finished.\n",
      "Now start the  9  th pair calculation.\n",
      "9 th has finished.\n",
      "Now start the  10  th pair calculation.\n",
      "10 th has finished.\n",
      "Now start the  11  th pair calculation.\n",
      "11 th has finished.\n",
      "Now start the  12  th pair calculation.\n",
      "12 th has finished.\n",
      "Now start the  13  th pair calculation.\n",
      "13 th has finished.\n",
      "Now start the  14  th pair calculation.\n",
      "14 th has finished.\n",
      "Now start the  15  th pair calculation.\n",
      "15 th has finished.\n",
      "Now start the  16  th pair calculation.\n",
      "16 th has finished.\n",
      "Now start the  17  th pair calculation.\n",
      "17 th has finished.\n",
      "Now start the  18  th pair calculation.\n",
      "18 th has finished.\n",
      "Now start the  19  th pair calculation.\n",
      "19 th has finished.\n",
      "Now start the  20  th pair calculation.\n",
      "20 th has finished.\n"
     ]
    }
   ],
   "source": [
    "# the final output dataframe\n",
    "score_dict = {'Plaintiff_Patent':[],'Defendant_Patent':[],'Plaintiff_Category':[],'Defendant_Category':[],'Similarity_Score':[]}\n",
    "score_df = pd.DataFrame(score_dict)\n",
    "# calculate the similarity dataframe for each pair and append them to the score_df\n",
    "n = 1\n",
    "for plaintiff_no, defendant_no in zip(df_train_no['plaintiff_patent'],df_train_no['defendant_patent']):\n",
    "    print('Now start the ',n,' th pair calculation.')\n",
    "    new_score_df = compare_two_patents_with_dict(plaintiff_no,defendant_no,patent_data,tokenizer,model)\n",
    "    score_df = score_df.append(new_score_df, ignore_index=True)\n",
    "    print(n,'th has finished.')\n",
    "    n+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e26405fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Plaintiff_Patent</th>\n",
       "      <th>Defendant_Patent</th>\n",
       "      <th>Plaintiff_Category</th>\n",
       "      <th>Defendant_Category</th>\n",
       "      <th>Similarity_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US4762129B1</td>\n",
       "      <td>US7909811B2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0.271154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US4762129B1</td>\n",
       "      <td>US7909811B2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Claim_Set_1</td>\n",
       "      <td>0.200510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US4762129B1</td>\n",
       "      <td>US7909811B2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Claim_Set_2</td>\n",
       "      <td>0.259635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US4762129B1</td>\n",
       "      <td>US7909811B2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Claim_Set_3</td>\n",
       "      <td>0.231864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US4762129B1</td>\n",
       "      <td>US7909811B2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Figures_Desc</td>\n",
       "      <td>0.256694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>US6413289B2</td>\n",
       "      <td>US20070084169A1</td>\n",
       "      <td>Disclosure</td>\n",
       "      <td>Claim_Set_1</td>\n",
       "      <td>0.764612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>US6413289B2</td>\n",
       "      <td>US20070084169A1</td>\n",
       "      <td>Disclosure</td>\n",
       "      <td>Claim_Set_2</td>\n",
       "      <td>0.778324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>US6413289B2</td>\n",
       "      <td>US20070084169A1</td>\n",
       "      <td>Disclosure</td>\n",
       "      <td>Claim_Set_3</td>\n",
       "      <td>0.783793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>US6413289B2</td>\n",
       "      <td>US20070084169A1</td>\n",
       "      <td>Disclosure</td>\n",
       "      <td>Figures_Desc</td>\n",
       "      <td>0.875504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>US6413289B2</td>\n",
       "      <td>US20070084169A1</td>\n",
       "      <td>Disclosure</td>\n",
       "      <td>Disclosure</td>\n",
       "      <td>0.838761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>673 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Plaintiff_Patent Defendant_Patent Plaintiff_Category Defendant_Category  \\\n",
       "0        US4762129B1      US7909811B2           Abstract           Abstract   \n",
       "1        US4762129B1      US7909811B2           Abstract        Claim_Set_1   \n",
       "2        US4762129B1      US7909811B2           Abstract        Claim_Set_2   \n",
       "3        US4762129B1      US7909811B2           Abstract        Claim_Set_3   \n",
       "4        US4762129B1      US7909811B2           Abstract       Figures_Desc   \n",
       "..               ...              ...                ...                ...   \n",
       "668      US6413289B2  US20070084169A1         Disclosure        Claim_Set_1   \n",
       "669      US6413289B2  US20070084169A1         Disclosure        Claim_Set_2   \n",
       "670      US6413289B2  US20070084169A1         Disclosure        Claim_Set_3   \n",
       "671      US6413289B2  US20070084169A1         Disclosure       Figures_Desc   \n",
       "672      US6413289B2  US20070084169A1         Disclosure         Disclosure   \n",
       "\n",
       "     Similarity_Score  \n",
       "0            0.271154  \n",
       "1            0.200510  \n",
       "2            0.259635  \n",
       "3            0.231864  \n",
       "4            0.256694  \n",
       "..                ...  \n",
       "668          0.764612  \n",
       "669          0.778324  \n",
       "670          0.783793  \n",
       "671          0.875504  \n",
       "672          0.838761  \n",
       "\n",
       "[673 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b26297f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T19:45:40.782284Z",
     "iopub.status.busy": "2022-03-02T19:45:40.782049Z",
     "iopub.status.idle": "2022-03-02T19:45:40.792316Z",
     "shell.execute_reply": "2022-03-02T19:45:40.791802Z",
     "shell.execute_reply.started": "2022-03-02T19:45:40.782261Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the score dataframe to csv\n",
    "score_df.to_csv('Pairs_Part_Scores350-369.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6cf1fe69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99316"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(patent_data['US8216209B2']['Figures_Desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0aa8b10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part use to clean the similarity vectors for the patent with float 'nan' Abstract\n",
    "import numpy as np\n",
    "\n",
    "def get_useless_set(patent_data):\n",
    "    useless_set = set()\n",
    "    for key in patent_data:\n",
    "        if type(patent_data[key]['Abstract'])==float:\n",
    "            useless_set.add(key)\n",
    "    return useless_set\n",
    "\n",
    "def clean_abstract(df,useless_set):\n",
    "    mask1 = []\n",
    "    mask2 = []\n",
    "    for i in range(len(df)):\n",
    "        if (df.iloc[i]['Plaintiff_Patent'] in useless_set) and (df.iloc[i]['Plaintiff_Category']=='Abstract'):\n",
    "            mask1.append(True)\n",
    "        else:\n",
    "            mask1.append(False)\n",
    "\n",
    "        if (df.iloc[i]['Defendant_Patent'] in useless_set) and (df.iloc[i]['Defendant_Category']=='Abstract'):\n",
    "            mask2.append(True)\n",
    "        else:\n",
    "            mask2.append(False)\n",
    "        \n",
    "    mask = np.array(mask1)+np.array(mask2)\n",
    "    \n",
    "    df_new = df[~mask]\n",
    "    \n",
    "    return df_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "08de4769",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_needtoclean = pd.read_csv('Pairs_Part_Scores0-999.csv')\n",
    "useless_set = get_useless_set(patent_data)\n",
    "df_clean = clean_abstract(df_needtoclean,useless_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "967cb8a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Plaintiff_Patent</th>\n",
       "      <th>Defendant_Patent</th>\n",
       "      <th>Plaintiff_Category</th>\n",
       "      <th>Defendant_Category</th>\n",
       "      <th>Similarity_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>US5897576A</td>\n",
       "      <td>WO2012063215A2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0.664770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>US5897576A</td>\n",
       "      <td>WO2012063215A2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Disclosure</td>\n",
       "      <td>0.735784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>US5897576A</td>\n",
       "      <td>WO2012063215A2</td>\n",
       "      <td>Claim_Set_1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0.607359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>US5897576A</td>\n",
       "      <td>WO2012063215A2</td>\n",
       "      <td>Claim_Set_1</td>\n",
       "      <td>Disclosure</td>\n",
       "      <td>0.754643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>US5897576A</td>\n",
       "      <td>WO2012063215A2</td>\n",
       "      <td>Claim_Set_2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0.587054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29338</th>\n",
       "      <td>1808</td>\n",
       "      <td>US5707392A</td>\n",
       "      <td>US20050235725A1</td>\n",
       "      <td>Disclosure</td>\n",
       "      <td>Claim_Set_2</td>\n",
       "      <td>0.791960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29339</th>\n",
       "      <td>1809</td>\n",
       "      <td>US5707392A</td>\n",
       "      <td>US20050235725A1</td>\n",
       "      <td>Disclosure</td>\n",
       "      <td>Claim_Set_3</td>\n",
       "      <td>0.744056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29340</th>\n",
       "      <td>1810</td>\n",
       "      <td>US5707392A</td>\n",
       "      <td>US20050235725A1</td>\n",
       "      <td>Disclosure</td>\n",
       "      <td>Claim_Set_4</td>\n",
       "      <td>0.756714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29341</th>\n",
       "      <td>1811</td>\n",
       "      <td>US5707392A</td>\n",
       "      <td>US20050235725A1</td>\n",
       "      <td>Disclosure</td>\n",
       "      <td>Figures_Desc</td>\n",
       "      <td>0.795352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29342</th>\n",
       "      <td>1812</td>\n",
       "      <td>US5707392A</td>\n",
       "      <td>US20050235725A1</td>\n",
       "      <td>Disclosure</td>\n",
       "      <td>Disclosure</td>\n",
       "      <td>0.849525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27598 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 Plaintiff_Patent Defendant_Patent Plaintiff_Category  \\\n",
       "0               0       US5897576A   WO2012063215A2           Abstract   \n",
       "1               1       US5897576A   WO2012063215A2           Abstract   \n",
       "2               2       US5897576A   WO2012063215A2        Claim_Set_1   \n",
       "3               3       US5897576A   WO2012063215A2        Claim_Set_1   \n",
       "4               4       US5897576A   WO2012063215A2        Claim_Set_2   \n",
       "...           ...              ...              ...                ...   \n",
       "29338        1808       US5707392A  US20050235725A1         Disclosure   \n",
       "29339        1809       US5707392A  US20050235725A1         Disclosure   \n",
       "29340        1810       US5707392A  US20050235725A1         Disclosure   \n",
       "29341        1811       US5707392A  US20050235725A1         Disclosure   \n",
       "29342        1812       US5707392A  US20050235725A1         Disclosure   \n",
       "\n",
       "      Defendant_Category  Similarity_Score  \n",
       "0               Abstract          0.664770  \n",
       "1             Disclosure          0.735784  \n",
       "2               Abstract          0.607359  \n",
       "3             Disclosure          0.754643  \n",
       "4               Abstract          0.587054  \n",
       "...                  ...               ...  \n",
       "29338        Claim_Set_2          0.791960  \n",
       "29339        Claim_Set_3          0.744056  \n",
       "29340        Claim_Set_4          0.756714  \n",
       "29341       Figures_Desc          0.795352  \n",
       "29342         Disclosure          0.849525  \n",
       "\n",
       "[27598 rows x 6 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fabb67ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv('test_train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dcb102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
